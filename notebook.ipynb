{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "In this notebook, we present the major experimental findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PE-UCRL in a polarising recommender engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the configurations file by altering the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config_files/peucrl_polarisation_1.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the notebook cell below, ```PilotExperimentation``` and ```gym_cellular``` have to be cloned as two separate directories in the same directory.\n",
    "The notebook cell reads necessary files, imports necessary packages, and instantiates the environment and agent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9,\n",
      " 'cell_classes': None,\n",
      " 'cell_labelling_function': None,\n",
      " 'confidence_level': 0.95,\n",
      " 'constraints_file_path': 'constraints/clinical_trial_0.props',\n",
      " 'environment_seed': 0,\n",
      " 'environment_version': 'gym_cellular/Polarisation-v2',\n",
      " 'max_time_steps': 30,\n",
      " 'n_moderators': 2,\n",
      " 'n_recommendations': 4,\n",
      " 'n_user_states': 5,\n",
      " 'n_users': 3,\n",
      " 'regulatory_constraints': None,\n",
      " 'reset_seed': 0}\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from agents import PeUcrlAgent\n",
    "from json import load\n",
    "import gymnasium as gym\n",
    "!cd ..; pip3 install -e gym-cellular -q\n",
    "import gym_cellular\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# import configurations\n",
    "config_file = open(config_file_path, 'r')\n",
    "config = load(config_file)\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make(\n",
    "    config[\"environment_version\"],\n",
    "    n_users=config[\"n_users\"],\n",
    "    n_user_states=config[\"n_user_states\"],\n",
    "    n_recommendations=config[\"n_recommendations\"],\n",
    "    n_moderators=config[\"n_moderators\"],\n",
    "    seed=config[\"environment_seed\"],\n",
    ")\n",
    "\n",
    "# initialise environment\n",
    "previous_state, info = env.reset(seed=config[\"reset_seed\"])\n",
    "\n",
    "# instantiate agent\n",
    "agt = PeUcrlAgent(\n",
    "    confidence_level=config[\"confidence_level\"],\n",
    "    accuracy=config[\"accuracy\"],\n",
    "    n_cells=config[\"n_users\"],\n",
    "    n_intracellular_states=config[\"n_user_states\"] * 2,\n",
    "    cellular_encoding=env.cellular_encoding,\n",
    "    n_intracellular_actions=config[\"n_recommendations\"],\n",
    "    cellular_decoding=env.cellular_decoding,\n",
    "    reward_function=env.tabular_reward_function,\n",
    "    cell_classes=config[\"cell_classes\"],\n",
    "    cell_labelling_function=config[\"cell_labelling_function\"],\n",
    "    regulatory_constraints=config[\"constraints_file_path\"],\n",
    "    initial_state=previous_state,\n",
    "    initial_policy=env.get_initial_policy(),\n",
    ")\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is run in the environment in the notebook cell below, and data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inintialisation\n",
      "state: {'polarisation': array([4, 4, 4]), 'two_way_polarisable': array([0, 1, 0])}\n",
      "\n",
      "Time step: 0\n",
      "action: [0 0 0]\n",
      "state: {'polarisation': array([3, 3, 3]), 'two_way_polarisable': array([0, 1, 0])}\n",
      "reward: 0.13250101235390163\n",
      "side effects:\n",
      "array([['safe', 'safe', 'silent'],\n",
      "       ['safe', 'safe', 'safe'],\n",
      "       ['silent', 'silent', 'silent']], dtype='<U6')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'agents/prism/output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m action \u001b[39m=\u001b[39m agt\u001b[39m.\u001b[39msample_action(previous_state)\n\u001b[1;32m     16\u001b[0m current_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> 17\u001b[0m agt\u001b[39m.\u001b[39;49mupdate(current_state, reward, info[\u001b[39m\"\u001b[39;49m\u001b[39mside_effects\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     19\u001b[0m \u001b[39m# save data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m reward_cumulation[time_step] \u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Gits/PilotExperimentation/agents/peucrl.py:173\u001b[0m, in \u001b[0;36mPeUcrlAgent.update\u001b[0;34m(self, current_state, reward, side_effects)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_confidence_sets()\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extended_value_iteration()\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pe_shield()\n\u001b[1;32m    174\u001b[0m \u001b[39m#self.current_episode_time_step = self.time_step # is this right? If not, could I just use the time step?\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_episodes_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_episode_count\n",
      "File \u001b[0;32m~/Gits/PilotExperimentation/agents/peucrl.py:423\u001b[0m, in \u001b[0;36mPeUcrlAgent._pe_shield\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m cell_set \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(cell)\n\u001b[1;32m    422\u001b[0m tmp_policy[cell, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_policy[cell, :]\n\u001b[0;32m--> 423\u001b[0m verified \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_verify(tmp_policy)\n\u001b[1;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verified:\n\u001b[1;32m    425\u001b[0m     tmp_policy[cell, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbehaviour_policy[cell, :]\n",
      "File \u001b[0;32m~/Gits/PilotExperimentation/agents/peucrl.py:442\u001b[0m, in \u001b[0;36mPeUcrlAgent._verify\u001b[0;34m(self, tmp_policy)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_verify\u001b[39m(\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    438\u001b[0m     tmp_policy,\n\u001b[1;32m    439\u001b[0m ):\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m     \u001b[39m#verified = True\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     verified \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprism_verify(tmp_policy)\n\u001b[1;32m    444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Gits/PilotExperimentation/agents/peucrl.py:466\u001b[0m, in \u001b[0;36mPeUcrlAgent.prism_verify\u001b[0;34m(self, tmp_policy)\u001b[0m\n\u001b[1;32m    464\u001b[0m system(\u001b[39m'\u001b[39m\u001b[39mrm -f output.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    465\u001b[0m system(\u001b[39m'\u001b[39m\u001b[39mcd agents/prism; prism model.prism constraints.props -param \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m param_arg[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m > output.txt\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[0;32m--> 466\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39magents/prism/output.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    467\u001b[0m     line_set \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m    468\u001b[0m     occurances \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'agents/prism/output.txt'"
     ]
    }
   ],
   "source": [
    "# initialise data-saving structures\n",
    "reward_cumulation = np.zeros(config[\"max_time_steps\"]) * np.nan\n",
    "side_effects_incidence = np.zeros(config[\"max_time_steps\"]) * np.nan\n",
    "ns_between_time_steps = np.zeros(config[\"max_time_steps\"]) * np.nan\n",
    "ns_between_episodes = np.zeros(config[\"max_time_steps\"]) * np.nan\n",
    "\n",
    "# print\n",
    "print(\"\")\n",
    "print(\"inintialisation\")\n",
    "print(\"state:\", previous_state)\n",
    "\n",
    "for time_step in range(config[\"max_time_steps\"]):\n",
    "\n",
    "    # interact\n",
    "    action = agt.sample_action(previous_state)\n",
    "    current_state, reward, terminated, truncated, info = env.step(action)\n",
    "    agt.update(current_state, reward, info[\"side_effects\"])\n",
    "\n",
    "    # save data\n",
    "    reward_cumulation[time_step] = reward\n",
    "    side_effects_incidence[time_step] = env.get_side_effects_incidence()\n",
    "    ns_between_time_steps[time_step] = agt.get_ns_between_time_steps()\n",
    "    ns_between_episodes[time_step] = agt.get_ns_between_episodes()\n",
    "\n",
    "    # print\n",
    "    if time_step <= 1 or terminated or truncated or time_step >= config[\"max_time_steps\"] - 2:\n",
    "        print(\"\")\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"action:\", action)\n",
    "        print(\"state:\", current_state)\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"side effects:\")\n",
    "        pprint(info[\"side_effects\"])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "    previous_state = current_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook cell below, side effects incidence and reward respectively are plotted against the number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "x = np.arange(0, config[\"max_time_steps\"])\n",
    "ax2.set_xlabel(\"time step\")\n",
    "ax1.set_ylabel(\"side effects incidence\")\n",
    "y = side_effects_incidence\n",
    "ax1.plot(x, y)\n",
    "ax2.set_ylabel(\"reward\")\n",
    "y = reward_cumulation\n",
    "ax2.plot(x, y)\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook cell below, a table is printed showing the time to perform the between time step computations as well as the between episodes computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    [np.nanmean(ns_between_time_steps)/1e9, np.nanstd(ns_between_time_steps)/1e9, np.count_nonzero(~np.isnan(ns_between_time_steps))],\n",
    "    [np.nanmean(ns_between_episodes)/1e9, np.nanstd(ns_between_episodes)/1e9, np.count_nonzero(~np.isnan(ns_between_episodes))],\n",
    "]\n",
    "df = pd.DataFrame(table, columns=[\"mean (s)\", \"standard devation (s)\", \"number of samples\"], index=[\"between time steps\", \"between episodes\"])\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f4a9dd39f8169da338793df97c04700c64fbec15d76fc5997a12166e019f5c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
