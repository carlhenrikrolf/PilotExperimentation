{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "In this notebook, we present the major experimental findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PE-UCRL in a polarising recommender engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the configurations file by altering the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config_files/peucrl_polarisation_1.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the notebook cell below, ```PilotExperimentation``` and ```gym_cellular``` have to be cloned as two separate directories in the same directory.\n",
    "The notebook cell reads necessary files, imports necessary packages, and instantiates the environment and agent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     17\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\n\u001b[1;32m     18\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39menvironment_version\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     n_users\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mn_users\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     seed\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39menvironment_seed\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[39m#def reward_function(x,y):\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#    return 0\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m# instantiate agent\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m agt \u001b[39m=\u001b[39m PeUcrlAgent(\n\u001b[1;32m     31\u001b[0m     confidence_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mconfidence_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     32\u001b[0m     accuracy\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     33\u001b[0m     n_cells\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mn_users\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     34\u001b[0m     n_intracellular_states\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mn_user_states\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     cellular_encoding\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49mcellular_encoding,\n\u001b[1;32m     36\u001b[0m     n_intracellular_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mn_recommendations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     37\u001b[0m     cellular_decoding\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49mcellular_decoding,\n\u001b[1;32m     38\u001b[0m     reward_function\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49mtabular_reward_function,\n\u001b[1;32m     39\u001b[0m     cell_classes\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcell_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     40\u001b[0m     cell_labelling_function\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcell_labelling_function\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     41\u001b[0m     regulatory_constraints\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mregulatory_constraints\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     42\u001b[0m     initial_policy\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49mget_initial_policy(),\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m pprint(config)\n",
      "File \u001b[0;32m~/Gits/PilotExperimentation/agents/peucrl.py:53\u001b[0m, in \u001b[0;36mPeUcrlAgent.__init__\u001b[0;34m(self, confidence_level, accuracy, n_cells, n_intracellular_states, cellular_encoding, n_intracellular_actions, cellular_decoding, cell_classes, cell_labelling_function, regulatory_constraints, initial_policy, reward_function)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m flat_state \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_states):\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m flat_action \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_actions):\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_function[flat_state,flat_action] \u001b[39m=\u001b[39m reward_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unflatten(flat_state\u001b[39m=\u001b[39;49mflat_state), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unflatten(flat_action\u001b[39m=\u001b[39;49mflat_action))\n\u001b[1;32m     55\u001b[0m \u001b[39m# initialise behaviour policy\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbehaviour_policy \u001b[39m=\u001b[39m initial_policy \u001b[39m# using some kind of conversion?\u001b[39;00m\n",
      "File \u001b[0;32m~/Gits/gym-cellular/gym_cellular/envs/polarisation_v2.py:292\u001b[0m, in \u001b[0;36mPolarisationV2Env.tabular_reward_function\u001b[0;34m(self, tabular_observation, tabular_action)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtabular_reward_function\u001b[39m(\u001b[39mself\u001b[39m, tabular_observation, tabular_action):\n\u001b[0;32m--> 292\u001b[0m \tobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inverse_tabular_encoding(tabular_observation)\n\u001b[1;32m    293\u001b[0m \taction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inverse_tabular_decoding(tabular_action)\n\u001b[1;32m    294\u001b[0m \treward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_function(observation, action)\n",
      "File \u001b[0;32m~/Gits/gym-cellular/gym_cellular/envs/polarisation_v2.py:248\u001b[0m, in \u001b[0;36mPolarisationV2Env._inverse_tabular_encoding\u001b[0;34m(self, tabular_observation)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inverse_tabular_encoding\u001b[39m(\u001b[39mself\u001b[39m, tabular_observation):\n\u001b[1;32m    246\u001b[0m \u001b[39m\t\u001b[39m\u001b[39m\"\"\"The inverse of tabular_encoding. Turns a tabular state representation into an observation that can be used as input to the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \t\u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tabular_observation) \u001b[39mis\u001b[39;00m \u001b[39mint\u001b[39m\n\u001b[1;32m    249\u001b[0m \t\u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tabular_observation \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_states\n\u001b[1;32m    251\u001b[0m \talist \u001b[39m=\u001b[39m tabular2cellular(tabular_observation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_user_states \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_users)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from agents import PeUcrlAgent\n",
    "from json import load\n",
    "import gymnasium as gym\n",
    "!cd ..; pip3 install -e gym-cellular -q\n",
    "import gym_cellular\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# import configurations\n",
    "config_file = open(config_file_path, 'r')\n",
    "config = load(config_file)\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make(\n",
    "    config[\"environment_version\"],\n",
    "    n_users=config[\"n_users\"],\n",
    "    n_user_states=config[\"n_user_states\"],\n",
    "    n_recommendations=config[\"n_recommendations\"],\n",
    "    n_moderators=config[\"n_moderators\"],\n",
    "    seed=config[\"environment_seed\"],\n",
    ")\n",
    "\n",
    "#def reward_function(x,y):\n",
    "#    return 0\n",
    "\n",
    "# instantiate agent\n",
    "agt = PeUcrlAgent(\n",
    "    confidence_level=config[\"confidence_level\"],\n",
    "    accuracy=config[\"accuracy\"],\n",
    "    n_cells=config[\"n_users\"],\n",
    "    n_intracellular_states=config[\"n_user_states\"] * 2,\n",
    "    cellular_encoding=env.cellular_encoding,\n",
    "    n_intracellular_actions=config[\"n_recommendations\"],\n",
    "    cellular_decoding=env.cellular_decoding,\n",
    "    reward_function=env.tabular_reward_function,\n",
    "    cell_classes=config[\"cell_classes\"],\n",
    "    cell_labelling_function=config[\"cell_labelling_function\"],\n",
    "    regulatory_constraints=config[\"regulatory_constraints\"],\n",
    "    initial_policy=env.get_initial_policy(),\n",
    ")\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is run in the environment in the notebook cell below, and data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise environment\n",
    "previous_state, info = env.reset(seed=config[\"reset_seed\"])\n",
    "\n",
    "# initialise data-saving structures\n",
    "reward_cumulation = np.zeros(config[\"max_time_steps\"])\n",
    "side_effects_incidence = np.zeros(config[\"max_time_steps\"])\n",
    "ns_between_time_steps = np.zeros(config[\"max_time_steps\"])\n",
    "ns_between_episodes = np.zeros(config[\"max_time_steps\"])\n",
    "\n",
    "for time_step in range(config[\"max_time_steps\"]):\n",
    "\n",
    "    # interact\n",
    "    action = agt.sample_action(previous_state)\n",
    "    current_state, reward, terminated, truncated, info = env.step(action)\n",
    "    agt.update(current_state, reward, info[\"side_effects\"])\n",
    "\n",
    "    # save data\n",
    "    reward_cumulation[time_step] = reward\n",
    "    side_effects_incidence[time_step] = env.get_side_effects_incidence()\n",
    "    ns_between_time_steps[time_step] = agt.get_ns_between_time_steps()\n",
    "    ns_between_episodes[time_step] = agt.get_ns_between_episodes()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "    previous_state = current_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook cell below, side effects incidence and reward respectively are plotted against the number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "x = np.arange(0, config[\"max_time_steps\"])\n",
    "ax2.set_xlabel(\"time step\")\n",
    "ax1.set_ylabel(\"side effects incidence\")\n",
    "y = side_effects_incidence\n",
    "ax1.plot(x, y)\n",
    "ax2.set_ylabel(\"reward\")\n",
    "y = reward_cumulation\n",
    "ax2.plot(x, y)\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook cell below, a table is printed showing the time to perform the between time step computations as well as the between episodes computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    [np.mean(ns_between_time_steps), np.std(ns_between_time_steps), config[\"max_time_steps\"]],\n",
    "    [np.nanmean(ns_between_episodes), np.nanstd(ns_between_episodes), np.count_nonzero(~np.isnan(ns_between_episodes))],\n",
    "]\n",
    "df = pd.DataFrame(table, columns=[\"mean (ns)\", \"standard devation (ns)\", \"number of samples\"], index=[\"between time steps\", \"between episodes\"])\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f4a9dd39f8169da338793df97c04700c64fbec15d76fc5997a12166e019f5c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
